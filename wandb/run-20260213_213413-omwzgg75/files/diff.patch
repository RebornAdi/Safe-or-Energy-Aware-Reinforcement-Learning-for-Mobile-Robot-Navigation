diff --git a/Evaluate_agents.py b/Evaluate_agents.py
index 5271a90..86397f9 100644
--- a/Evaluate_agents.py
+++ b/Evaluate_agents.py
@@ -2,62 +2,32 @@ import numpy as np
 from stable_baselines3 import PPO
 from safe_energy_nav_env import SafeEnergyNavEnv
 
-def evaluate_model(model_path, use_safety, use_energy, episodes=50):
-    env = SafeEnergyNavEnv(
-        use_safety=use_safety,
-        use_energy=use_energy
-    )
+def evaluate(model_path, use_safety, use_energy, episodes=50):
 
+    env = SafeEnergyNavEnv(use_safety=use_safety, use_energy=use_energy)
     model = PPO.load(model_path)
 
-    success_count = 0
-    collision_count = 0
-    total_energy = 0
-    total_path = 0
+    success = 0
+    collisions = 0
 
-    for ep in range(episodes):
+    for _ in range(episodes):
         obs, _ = env.reset()
         done = False
-        prev_position = env.position.copy()
 
         while not done:
             action, _ = model.predict(obs, deterministic=True)
             obs, reward, terminated, truncated, _ = env.step(action)
 
-            # Energy
-            total_energy += (env.v**2 + env.omega**2)
-
-            # Path length
-            step_dist = np.linalg.norm(env.position - prev_position)
-            total_path += step_dist
-            prev_position = env.position.copy()
-
-            if terminated and np.linalg.norm(env.goal - env.position) < 0.3:
-                success_count += 1
-
-            if terminated and np.linalg.norm(env.obstacle - env.position) < env.collision_distance:
-                collision_count += 1
+            if terminated:
+                if np.linalg.norm(env.goal - env.position) < 0.3:
+                    success += 1
+                if np.linalg.norm(env.obstacle - env.position) < env.collision_distance:
+                    collisions += 1
 
             done = terminated or truncated
 
-    env.close()
-
-    print(f"\nResults for {model_path}")
-    print(f"Success Rate: {success_count/episodes:.2f}")
-    print(f"Collisions: {collision_count}")
-    print(f"Average Energy: {total_energy/episodes:.2f}")
-    print(f"Average Path Length: {total_path/episodes:.2f}")
-    print("-"*40)
-
+    print("Success Rate:", success / episodes)
+    print("Collisions:", collisions)
 
 if __name__ == "__main__":
-
-    evaluate_model("ppo_baseline", False, False)
-    evaluate_model("ppo_safe", True, False)
-    evaluate_model("ppo_energy", False, True)
-    evaluate_model("ppo_ours", True, True)
-# ---------------- SELECT AGENT ----------------
-# Baseline: use_safety=False, use_energy=False
-# Safe:     use_safety=True,  use_energy=False
-# Energy:   use_safety=False, use_energy=True
-# OURS:     use_safety=True,  use_energy=True
\ No newline at end of file
+    evaluate("ppo_ours", True, True)
diff --git a/__pycache__/safe_energy_nav_env.cpython-312.pyc b/__pycache__/safe_energy_nav_env.cpython-312.pyc
index 96098cf..9d1f770 100644
Binary files a/__pycache__/safe_energy_nav_env.cpython-312.pyc and b/__pycache__/safe_energy_nav_env.cpython-312.pyc differ
diff --git a/ppo_baseline.zip b/ppo_baseline.zip
index 0f643c9..91958e0 100644
Binary files a/ppo_baseline.zip and b/ppo_baseline.zip differ
diff --git a/ppo_energy.zip b/ppo_energy.zip
index b7c1ee0..8457560 100644
Binary files a/ppo_energy.zip and b/ppo_energy.zip differ
diff --git a/ppo_ours.zip b/ppo_ours.zip
index 571a9f1..be2aa9e 100644
Binary files a/ppo_ours.zip and b/ppo_ours.zip differ
diff --git a/ppo_safe.zip b/ppo_safe.zip
index f7df36f..873957d 100644
Binary files a/ppo_safe.zip and b/ppo_safe.zip differ
diff --git a/safe_energy_nav_env.py b/safe_energy_nav_env.py
index 970c908..84d79e1 100644
--- a/safe_energy_nav_env.py
+++ b/safe_energy_nav_env.py
@@ -4,7 +4,7 @@ import numpy as np
 import matplotlib.pyplot as plt
 
 class SafeEnergyNavEnv(gym.Env):
-    metadata = {"render_modes": ["human"]}
+    metadata = {"render_modes": ["human"], "render_fps": 30}
 
     def __init__(self, use_safety=True, use_energy=True, render_mode=None):
         super().__init__()
@@ -13,71 +13,107 @@ class SafeEnergyNavEnv(gym.Env):
         self.use_energy = use_energy
         self.render_mode = render_mode
 
-        self.action_space = spaces.Box(low=-1.0,high=1.0,shape=(2,),dtype=np.float32)
+        self.world_size = 10.0
+        self.dt = 0.1
+        self.max_steps = 300
 
+        self.max_v = 1.0
+        self.max_omega = 1.0
 
-        self.observation_space = spaces.Box(
-            low=np.array([0.0, -np.pi, 0.0, 0.0, -1.0]),
-            high=np.array([15.0, np.pi, 15.0, 1.0, 1.0]),
+        self.safe_distance = 0.8
+        self.collision_distance = 0.3
+
+        self.action_space = spaces.Box(
+            low=-1.0,
+            high=1.0,
+            shape=(2,),
             dtype=np.float32
         )
 
-        self.world_size = 10.0
-        self.dt = 0.1
-        self.max_steps = 300
-
-        self.safe_distance = 0.6
-        self.collision_distance = 0.25
+        self.observation_space = spaces.Box(
+            low=np.array([0.0, -1.0, -1.0, 0.0, 0.0, -1.0]),
+            high=np.array([15.0, 1.0, 1.0, 15.0, 1.0, 1.0]),
+            dtype=np.float32
+        )
 
         self.fig = None
         self.ax = None
 
-        self.reset()
-
     def reset(self, seed=None, options=None):
         super().reset(seed=seed)
 
-        self.position = np.array([1.0, 1.0], dtype=np.float32)
-        self.heading = 0.0
-        self.v = 0.0
-        self.omega = 0.0
+        self.goal = self._random_pos()
+        self.obstacle = self._random_pos()
 
-        self.goal = np.array([8.5, 8.5], dtype=np.float32)
-        self.obstacle = np.array([4.5, 4.5], dtype=np.float32)
+        while np.linalg.norm(self.goal - self.obstacle) < 1.5:
+            self.obstacle = self._random_pos()
 
+        self.position = self._random_pos()
+        while (
+            np.linalg.norm(self.position - self.obstacle) < 1.0 or
+            np.linalg.norm(self.position - self.goal) < 1.0
+        ):
+            self.position = self._random_pos()
+
+        self.heading = self.np_random.uniform(-np.pi, np.pi)
+        self.v = 0.0
+        self.omega = 0.0
         self.steps = 0
+
         return self._get_obs(), {}
 
+    def _random_pos(self):
+        return self.np_random.uniform(1.0, self.world_size - 1.0, size=(2,)).astype(np.float32)
+
     def step(self, action):
         self.steps += 1
 
-        self.v = float(np.clip(action[0], 0.0, 1.0))
-        self.omega = float(np.clip(action[1], -1.0, 1.0))
+        self.v = ((action[0] + 1.0) / 2.0) * self.max_v
+        self.omega = action[1] * self.max_omega
+
+        prev_position = self.position.copy()
 
         self.heading += self.omega * self.dt
+        self.heading = (self.heading + np.pi) % (2*np.pi) - np.pi
+
         dx = self.v * np.cos(self.heading) * self.dt
         dy = self.v * np.sin(self.heading) * self.dt
+
         self.position += np.array([dx, dy])
+        self.position = np.clip(self.position, 0, self.world_size)
 
         dist_goal = np.linalg.norm(self.goal - self.position)
+        prev_dist_goal = np.linalg.norm(self.goal - prev_position)
         dist_obs = np.linalg.norm(self.obstacle - self.position)
 
-        reward = -dist_goal
+        reward = 0.0
+
+        # Progress shaping
+        reward += 5.0 * (prev_dist_goal - dist_goal)
 
+        # Small time penalty
+        reward -= 0.01
+
+        # Safety penalty
         if self.use_safety:
             if dist_obs < self.collision_distance:
-                reward -= 50.0
+                reward -= 100.0
             elif dist_obs < self.safe_distance:
-                reward -= 5.0 * (self.safe_distance - dist_obs)
+                penalty_factor = (
+                    (self.safe_distance - dist_obs) /
+                    (self.safe_distance - self.collision_distance)
+                )
+                reward -= 10.0 * penalty_factor
 
+        # Energy penalty
         if self.use_energy:
-            reward -= 0.5 * (self.v ** 2 + self.omega ** 2)
+            reward -= 0.02 * (self.v**2 + 0.3 * self.omega**2)
 
         terminated = False
         truncated = False
 
         if dist_goal < 0.3:
-            reward += 100.0
+            reward += 200.0
             terminated = True
 
         if dist_obs < self.collision_distance:
@@ -86,54 +122,20 @@ class SafeEnergyNavEnv(gym.Env):
         if self.steps >= self.max_steps:
             truncated = True
 
-        if self.render_mode == "human":
-            self.render()
-
         return self._get_obs(), float(reward), terminated, truncated, {}
 
     def _get_obs(self):
         vec_goal = self.goal - self.position
         heading_goal = np.arctan2(vec_goal[1], vec_goal[0])
+
         heading_error = heading_goal - self.heading
+        heading_error = (heading_error + np.pi) % (2*np.pi) - np.pi
 
         return np.array([
             np.linalg.norm(vec_goal),
-            heading_error,
+            np.cos(heading_error),
+            np.sin(heading_error),
             np.linalg.norm(self.obstacle - self.position),
             self.v,
             self.omega
         ], dtype=np.float32)
-
-    def render(self):
-        if self.fig is None:
-            plt.ion()
-            self.fig, self.ax = plt.subplots(figsize=(6, 6))
-
-        self.ax.clear()
-        self.ax.set_xlim(0, self.world_size)
-        self.ax.set_ylim(0, self.world_size)
-
-        self.ax.plot(self.position[0], self.position[1], "bo", label="Robot")
-
-        hx = self.position[0] + 0.5 * np.cos(self.heading)
-        hy = self.position[1] + 0.5 * np.sin(self.heading)
-        self.ax.plot([self.position[0], hx], [self.position[1], hy], "b-")
-
-        self.ax.plot(self.goal[0], self.goal[1], "g*", markersize=15, label="Goal")
-
-        obs = plt.Circle(self.obstacle, self.collision_distance, color="r")
-        self.ax.add_patch(obs)
-
-        safe = plt.Circle(
-            self.obstacle, self.safe_distance,
-            color="r", linestyle="--", fill=False
-        )
-        self.ax.add_patch(safe)
-
-        self.ax.set_title("Safe & Energy-Aware RL Navigation")
-        self.ax.legend()
-        plt.pause(0.001)
-
-    def close(self):
-        if self.fig:
-            plt.close(self.fig)
diff --git a/train_ppo.py b/train_ppo.py
index 8cb76d7..fdba533 100644
--- a/train_ppo.py
+++ b/train_ppo.py
@@ -1,36 +1,68 @@
 import torch
+import wandb
+from wandb.integration.sb3 import WandbCallback
 from stable_baselines3 import PPO
-from stable_baselines3.common.env_checker import check_env
+from stable_baselines3.common.vec_env import DummyVecEnv
+from stable_baselines3.common.monitor import Monitor
 from safe_energy_nav_env import SafeEnergyNavEnv
 
-print("Torch version:", torch.__version__)
-print("CUDA available:", torch.cuda.is_available())
-print("GPU:", torch.cuda.get_device_name(0))
-
 # ---------------- SELECT AGENT ----------------
-# Baseline: use_safety=False, use_energy=False
-# Safe:     use_safety=True,  use_energy=False
-# Energy:   use_safety=False, use_energy=True
-# OURS:     use_safety=True,  use_energy=True
+USE_SAFETY = False
+USE_ENERGY = False
+RUN_NAME = "ppo_baseline"
 
-env = SafeEnergyNavEnv(use_safety=True, use_energy=True)
+# ---------------- INIT WANDB ----------------
+run = wandb.init(
+    project="Safe-Energy-RL",
+    name=RUN_NAME,
+    config={
+        "algorithm": "PPO",
+        "learning_rate": 3e-4,
+        "n_steps": 4096,
+        "batch_size": 256,
+        "gamma": 0.99,
+        "use_safety": USE_SAFETY,
+        "use_energy": USE_ENERGY,
+    },
+    sync_tensorboard=True,
+    monitor_gym=True,
+    save_code=True,
+)
 
-check_env(env)
+print("CUDA:", torch.cuda.is_available())
+print("GPU:", torch.cuda.get_device_name(0))
+
+def make_env():
+    env = SafeEnergyNavEnv(use_safety=USE_SAFETY, use_energy=USE_ENERGY)
+    return Monitor(env)
+
+env = DummyVecEnv([make_env])
 
 model = PPO(
     "MlpPolicy",
     env,
     learning_rate=3e-4,
-    n_steps=2048,
-    batch_size=128,
+    n_steps=4096,
+    batch_size=256,
     n_epochs=10,
     gamma=0.99,
-    device="cuda",                 # ðŸ”¥ GPU ENABLED
-    verbose=1,
-    policy_kwargs=dict(net_arch=[256, 256])
+    gae_lambda=0.95,
+    clip_range=0.2,
+    ent_coef=0.01,
+    device="cuda",
+    tensorboard_log=f"runs/{run.id}",
+    policy_kwargs=dict(net_arch=[256, 256]),
+    verbose=1
 )
 
-model.learn(total_timesteps=500_000)
-model.save("ppo_ours")
+model.learn(
+    total_timesteps=1_000_000,
+    callback=WandbCallback(
+        gradient_save_freq=100,
+        model_save_path=f"models/{run.id}",
+        verbose=2,
+    )
+)
 
-env.close()
+model.save(RUN_NAME)
+run.finish()
diff --git a/visualise_policy.py b/visualise_policy.py
index e56e208..a7e18ed 100644
--- a/visualise_policy.py
+++ b/visualise_policy.py
@@ -1,13 +1,23 @@
 from stable_baselines3 import PPO
 from safe_energy_nav_env import SafeEnergyNavEnv
 
-env = SafeEnergyNavEnv(
-    use_safety=True,
-    use_energy=True,
-    render_mode="human"
-)
+env = SafeEnergyNavEnv(use_safety=True, use_energy=True, render_mode="human")
+model = PPO.load("ppo_ours")
 
-model = PPO.load("ppo_safe")
+obs, _ = env.reset()
+done = False
+
+while not done:
+    action, _ = model.predict(obs, deterministic=True)
+    obs, reward, terminated, truncated, _ = env.step(action)
+    done = terminated or truncated
+
+env.close()
+from stable_baselines3 import PPO
+from safe_energy_nav_env import SafeEnergyNavEnv
+
+env = SafeEnergyNavEnv(use_safety=True, use_energy=True, render_mode="human")
+model = PPO.load("ppo_ours")
 
 obs, _ = env.reset()
 done = False
